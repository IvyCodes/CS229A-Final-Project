\relax 
\citation{neal}
\citation{sample-recording}
\citation{li}
\citation{somervuo}
\citation{li}
\citation{neal}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{1}}
\citation{sample-recording}
\citation{neal}
\citation{xu}
\citation{li}
\@writefile{toc}{\contentsline {section}{\numberline {III}Dataset}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Methods (1-1.5 pages)}{2}}
\newlabel{hyp}{{2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Result of post-processing the predictions given by logistic regression. The horizontal axis represents the audio samples ordered temporally.The first plot is the ground truth, which is provided for the reader's convenience. Then in descending order, the plots correspond to the predictions, smoothed predictions, results, and smoothed results.}}{3}}
\newlabel{steps}{{1}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Experiments, Results, and Discussion}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-A}}Tuning the number of training iterations and learning rate $\alpha $}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-B}}Tuning the regularization constant $\lambda $}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Learning curve with $\alpha = 0.3$ across 2000 iterations of gradient descent. The asymptotic behavior at the number of iterations increase show that 2000 iterations is sufficient.}}{3}}
\newlabel{lcurve}{{2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Test and validation errors are plotted against each other to determine the best value of $\lambda $. Validation error is minimized at $\lambda = 0.1$, so this is chosen to be the regularization constant.}}{3}}
\newlabel{lambda}{{3}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-C}}Results}{3}}
\citation{neil}
\citation{neil}
\citation{somervuo}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Performance of trained model on training set.}}{4}}
\newlabel{train}{{4}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Performance of trained model on validation set.}}{4}}
\newlabel{val}{{5}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces This confusion matrix shows the results of the trained model applied to the test set.}}{4}}
\newlabel{confusion}{{6}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {V-C}1}Accuracy Metric}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {V-C}2}AUC Metric}{4}}
\bibcite{sample-recording}{1}
\bibcite{li}{2}
\bibcite{neal}{3}
\bibcite{somervuo}{4}
\bibcite{xu}{5}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces ROC curves of our algorithm compared to other algorithms show that logistic regression on MFCC features performs as well as state-of-the-art techniques.}}{5}}
\newlabel{roc}{{7}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The smoothed result of the trained model on an imbalanced data set suggests that the algorithm may generalize well to other sets of data.}}{5}}
\newlabel{imbalanced}{{8}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion and Future Work}{5}}
\@writefile{toc}{\contentsline {section}{References}{5}}
